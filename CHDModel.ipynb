{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CHDModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrandonSin/CPSC501Assignment4/blob/master/CHDModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1gCrXSWJMnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCTvjLqFJYRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "import sys\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow_core.python.keras import regularizers\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gje0p3es7m80",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "75fc821a-5a5b-414e-da25-19f55129448f"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a39c70ed-2f63-4c3e-ab84-a8ab524d9dfa\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-a39c70ed-2f63-4c3e-ab84-a8ab524d9dfa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving heart_test.csv to heart_test.csv\n",
            "Saving heart_train.csv to heart_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ4WVuN1U_VP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file = \"heart_train.csv\"\n",
        "test_file = \"heart_test.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2BDJMu6J42R",
        "colab_type": "code",
        "outputId": "7a0bac5f-4f87-4bfe-d896-d5c8bde9132d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "!head {train_file}"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sbp,tobacco,ldl,adiposity,famhist,typea,obesity,alcohol,age,chd\r\n",
            "160,12,5.73,23.11,Present,49,25.3,97.2,52,1\r\n",
            "144,0.01,4.41,28.61,Absent,55,28.87,2.06,63,1\r\n",
            "118,0.08,3.48,32.28,Present,52,29.14,3.81,46,0\r\n",
            "170,7.5,6.41,38.03,Present,51,31.99,24.26,58,1\r\n",
            "134,13.6,3.5,27.78,Present,60,25.99,57.34,49,1\r\n",
            "132,6.2,6.47,36.21,Present,62,30.77,14.14,45,0\r\n",
            "142,4.05,3.38,16.2,Absent,59,20.81,2.62,38,0\r\n",
            "114,4.08,4.59,14.6,Present,62,23.11,6.72,58,1\r\n",
            "114,0,3.83,19.4,Present,49,24.86,2.49,29,0\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01KRV62YJ5bu",
        "colab_type": "code",
        "outputId": "9c2b7ca3-f933-4da4-b2c8-b36b37b7b806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "!head {test_file}"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sbp,tobacco,ldl,adiposity,famhist,typea,obesity,alcohol,age,chd\r\n",
            "160,12,5.73,23.11,Present,49,25.3,97.2,52,1\r\n",
            "144,0.01,4.41,28.61,Absent,55,28.87,2.06,63,1\r\n",
            "118,0.08,3.48,32.28,Present,52,29.14,3.81,46,0\r\n",
            "170,7.5,6.41,38.03,Present,51,31.99,24.26,58,1\r\n",
            "134,13.6,3.5,27.78,Present,60,25.99,57.34,49,1\r\n",
            "132,6.2,6.47,36.21,Present,62,30.77,14.14,45,0\r\n",
            "142,4.05,3.38,16.2,Absent,59,20.81,2.62,38,0\r\n",
            "114,4.08,4.59,14.6,Present,62,23.11,6.72,58,1\r\n",
            "114,0,3.83,19.4,Present,49,24.86,2.49,29,0\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUhMX9n6Z6ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_batch(dataset):\n",
        "  for batch, label in dataset.take(1):\n",
        "    for key, value in batch.items():\n",
        "      print(\"{:20s}: {}\".format(key,value.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zFwz2hibTn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PackNumericFeatures(object):\n",
        "  def __init__(self,names):\n",
        "    self.names = names\n",
        "  \n",
        "  def __call__(self, features, labels):\n",
        "    numeric_features = [features.pop(name) for name in self.names]\n",
        "    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
        "    numeric_features = tf.stack(numeric_features, axis=-1)\n",
        "    features['numeric'] = numeric_features\n",
        "\n",
        "\n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC_awOYIbkj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset(file, **kwargs):\n",
        "  dataset = tf.data.experimental.make_csv_dataset(\n",
        "      file,\n",
        "      batch_size = 50,\n",
        "      label_name=LABEL_COLUMN,\n",
        "      na_value =\"?\",\n",
        "      num_epochs=1,\n",
        "      ignore_errors=True,\n",
        "      **kwargs)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A82rug65KeSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_COLUMN = 'chd'\n",
        "LABELS = [0, 1]\n",
        "\n",
        "CATEGORIES = {'famhist': ['Present', 'Absent']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGiKzLPOKfuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = \"heart_train.csv\"\n",
        "test_data = \"heart_test.csv\"\n",
        "\n",
        "SELECT_COLUMNS = ['sbp','tobacco','ldl','adiposity','famhist', 'typea','obesity','alcohol','age','chd']\n",
        "raw_train_data = get_dataset(train_data, select_columns=SELECT_COLUMNS)\n",
        "raw_test_data = get_dataset(test_data, select_columns=SELECT_COLUMNS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qole6DDLdTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch, label_batch = next(iter(raw_train_data))\n",
        "test_batch, label_batch = next(iter(raw_test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enGCrnE-EvPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzKaCc2ULlGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUMERIC_FEATURES = ['sbp','tobacco','ldl','adiposity', 'typea','obesity','alcohol','age']\n",
        "\n",
        "packed_train_data = raw_train_data.map(\n",
        "    PackNumericFeatures(NUMERIC_FEATURES))\n",
        "\n",
        "packed_test_data = raw_test_data.map(\n",
        "    PackNumericFeatures(NUMERIC_FEATURES))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMMGtsigLtGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch, label_batch = next(iter(packed_train_data))\n",
        "test_batch, label_batch = next(iter(packed_test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riXRIWkALx2a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "33c96437-b40f-4c24-c5ef-b590f90194da"
      },
      "source": [
        "import pandas as pd\n",
        "desc = pd.read_csv(train_data)[NUMERIC_FEATURES].describe()\n",
        "desc"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sbp</th>\n",
              "      <th>tobacco</th>\n",
              "      <th>ldl</th>\n",
              "      <th>adiposity</th>\n",
              "      <th>typea</th>\n",
              "      <th>obesity</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>138.326840</td>\n",
              "      <td>3.635649</td>\n",
              "      <td>4.740325</td>\n",
              "      <td>25.406732</td>\n",
              "      <td>53.103896</td>\n",
              "      <td>26.044113</td>\n",
              "      <td>17.044394</td>\n",
              "      <td>42.816017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>20.496317</td>\n",
              "      <td>4.593024</td>\n",
              "      <td>2.070909</td>\n",
              "      <td>7.780699</td>\n",
              "      <td>9.817534</td>\n",
              "      <td>4.213680</td>\n",
              "      <td>24.481059</td>\n",
              "      <td>14.608956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>101.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>6.740000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>14.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>124.000000</td>\n",
              "      <td>0.052500</td>\n",
              "      <td>3.282500</td>\n",
              "      <td>19.775000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>22.985000</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>31.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>134.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.340000</td>\n",
              "      <td>26.115000</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>25.805000</td>\n",
              "      <td>7.510000</td>\n",
              "      <td>45.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>148.000000</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>5.790000</td>\n",
              "      <td>31.227500</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>28.497500</td>\n",
              "      <td>23.892500</td>\n",
              "      <td>55.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>218.000000</td>\n",
              "      <td>31.200000</td>\n",
              "      <td>15.330000</td>\n",
              "      <td>42.490000</td>\n",
              "      <td>78.000000</td>\n",
              "      <td>46.580000</td>\n",
              "      <td>147.190000</td>\n",
              "      <td>64.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              sbp     tobacco         ldl  ...     obesity     alcohol         age\n",
              "count  462.000000  462.000000  462.000000  ...  462.000000  462.000000  462.000000\n",
              "mean   138.326840    3.635649    4.740325  ...   26.044113   17.044394   42.816017\n",
              "std     20.496317    4.593024    2.070909  ...    4.213680   24.481059   14.608956\n",
              "min    101.000000    0.000000    0.980000  ...   14.700000    0.000000   15.000000\n",
              "25%    124.000000    0.052500    3.282500  ...   22.985000    0.510000   31.000000\n",
              "50%    134.000000    2.000000    4.340000  ...   25.805000    7.510000   45.000000\n",
              "75%    148.000000    5.500000    5.790000  ...   28.497500   23.892500   55.000000\n",
              "max    218.000000   31.200000   15.330000  ...   46.580000  147.190000   64.000000\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmFVmsqmL6FU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MEAN = np.array(desc.T['mean'])\n",
        "STD = np.array(desc.T['std'])\n",
        "def normalize_numeric_data(data, mean, std):\n",
        "  return (data-mean)/std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIG6dDspMHan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
        "\n",
        "numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=normalizer, shape=[len(NUMERIC_FEATURES)])\n",
        "numeric_columns = [numeric_column]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8b1KSi7MMBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch['numeric']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7_Sg_x0MODw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)\n",
        "numeric_layer(train_batch).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BXNmG65MXZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_columns = []\n",
        "for feature, vocab in CATEGORIES.items():\n",
        "  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "        key=feature, vocabulary_list=vocab)\n",
        "  categorical_columns.append(tf.feature_column.indicator_column(cat_col))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pALQEo_jMfuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n",
        "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numeric_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FC9IMOiMnQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  preprocessing_layer,\n",
        "  layers.Dense(512, activation='relu'),\n",
        "  layers.Dropout(0.5),\n",
        "   layers.Dense(512, activation='relu'),\n",
        "  layers.Dropout(0.5),\n",
        "   layers.Dense(512, activation='selu'),\n",
        "  layers.Dropout(0.5),\n",
        "  layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkG6eoWaMqeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = packed_train_data.shuffle(500)\n",
        "test_data = packed_test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6H_AExPMsa5",
        "colab_type": "code",
        "outputId": "f31ca08e-4ae4-4c7a-ca20-f94b5fff785f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "print(\"--Train--\")\n",
        "model.fit(train_data, epochs=300)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "\n",
        "print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy * 100))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--Train--\n",
            "Epoch 1/300\n",
            "10/10 [==============================] - 1s 141ms/step - loss: 0.6403 - accuracy: 0.6537\n",
            "Epoch 2/300\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 0.5528 - accuracy: 0.7359\n",
            "Epoch 3/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.5778 - accuracy: 0.6991\n",
            "Epoch 4/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.5571 - accuracy: 0.7078\n",
            "Epoch 5/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.5332 - accuracy: 0.7316\n",
            "Epoch 6/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.5051 - accuracy: 0.7424\n",
            "Epoch 7/300\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.5180 - accuracy: 0.7294\n",
            "Epoch 8/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.5522 - accuracy: 0.7359\n",
            "Epoch 9/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.5166 - accuracy: 0.7294\n",
            "Epoch 10/300\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.5049 - accuracy: 0.7532\n",
            "Epoch 11/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.4722 - accuracy: 0.7576\n",
            "Epoch 12/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.5192 - accuracy: 0.7532\n",
            "Epoch 13/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.4988 - accuracy: 0.7532\n",
            "Epoch 14/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.4824 - accuracy: 0.7641\n",
            "Epoch 15/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.4704 - accuracy: 0.7446\n",
            "Epoch 16/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.4962 - accuracy: 0.7554\n",
            "Epoch 17/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.4578 - accuracy: 0.7641\n",
            "Epoch 18/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.4828 - accuracy: 0.7489\n",
            "Epoch 19/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.5040 - accuracy: 0.7554\n",
            "Epoch 20/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.4890 - accuracy: 0.7749\n",
            "Epoch 21/300\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 0.4634 - accuracy: 0.7792\n",
            "Epoch 22/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.5310 - accuracy: 0.7814\n",
            "Epoch 23/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.4761 - accuracy: 0.7814\n",
            "Epoch 24/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.4801 - accuracy: 0.7641\n",
            "Epoch 25/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.4651 - accuracy: 0.7814\n",
            "Epoch 26/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.4873 - accuracy: 0.7814\n",
            "Epoch 27/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.4363 - accuracy: 0.7857\n",
            "Epoch 28/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.4606 - accuracy: 0.7835\n",
            "Epoch 29/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.4252 - accuracy: 0.8095\n",
            "Epoch 30/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.4666 - accuracy: 0.7792\n",
            "Epoch 31/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.4185 - accuracy: 0.8139\n",
            "Epoch 32/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.4300 - accuracy: 0.7922\n",
            "Epoch 33/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.4394 - accuracy: 0.7922\n",
            "Epoch 34/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.4445 - accuracy: 0.7857\n",
            "Epoch 35/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.4241 - accuracy: 0.7900\n",
            "Epoch 36/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.4088 - accuracy: 0.7965\n",
            "Epoch 37/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.4210 - accuracy: 0.7857\n",
            "Epoch 38/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.4096 - accuracy: 0.8117\n",
            "Epoch 39/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.4052 - accuracy: 0.8117\n",
            "Epoch 40/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.4535 - accuracy: 0.8030\n",
            "Epoch 41/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.3988 - accuracy: 0.8160\n",
            "Epoch 42/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.3773 - accuracy: 0.8333\n",
            "Epoch 43/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.4012 - accuracy: 0.8182\n",
            "Epoch 44/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.3726 - accuracy: 0.8377\n",
            "Epoch 45/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.3975 - accuracy: 0.8009\n",
            "Epoch 46/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.3437 - accuracy: 0.8398\n",
            "Epoch 47/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.4110 - accuracy: 0.8030\n",
            "Epoch 48/300\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.3966 - accuracy: 0.8052\n",
            "Epoch 49/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.3532 - accuracy: 0.8377\n",
            "Epoch 50/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.3882 - accuracy: 0.8117\n",
            "Epoch 51/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.3938 - accuracy: 0.8247\n",
            "Epoch 52/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.4119 - accuracy: 0.8225\n",
            "Epoch 53/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.3546 - accuracy: 0.8333\n",
            "Epoch 54/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.3355 - accuracy: 0.8420\n",
            "Epoch 55/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.3456 - accuracy: 0.8290\n",
            "Epoch 56/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.3130 - accuracy: 0.8463\n",
            "Epoch 57/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.3583 - accuracy: 0.8420\n",
            "Epoch 58/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.3502 - accuracy: 0.8506\n",
            "Epoch 59/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.3573 - accuracy: 0.8550\n",
            "Epoch 60/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.3539 - accuracy: 0.8398\n",
            "Epoch 61/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.3320 - accuracy: 0.8463\n",
            "Epoch 62/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.3289 - accuracy: 0.8463\n",
            "Epoch 63/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.3514 - accuracy: 0.8420\n",
            "Epoch 64/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.2992 - accuracy: 0.8485\n",
            "Epoch 65/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.3106 - accuracy: 0.8658\n",
            "Epoch 66/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.3255 - accuracy: 0.8766\n",
            "Epoch 67/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.3376 - accuracy: 0.8485\n",
            "Epoch 68/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.3302 - accuracy: 0.8593\n",
            "Epoch 69/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.3187 - accuracy: 0.8571\n",
            "Epoch 70/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.2756 - accuracy: 0.8939\n",
            "Epoch 71/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.3375 - accuracy: 0.8723\n",
            "Epoch 72/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.3065 - accuracy: 0.8571\n",
            "Epoch 73/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.2543 - accuracy: 0.8961\n",
            "Epoch 74/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.3313 - accuracy: 0.8810\n",
            "Epoch 75/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.3169 - accuracy: 0.8723\n",
            "Epoch 76/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.2835 - accuracy: 0.8918\n",
            "Epoch 77/300\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 0.2615 - accuracy: 0.8745\n",
            "Epoch 78/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.3035 - accuracy: 0.8615\n",
            "Epoch 79/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.2781 - accuracy: 0.8896\n",
            "Epoch 80/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2760 - accuracy: 0.8701\n",
            "Epoch 81/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2757 - accuracy: 0.8723\n",
            "Epoch 82/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2723 - accuracy: 0.8810\n",
            "Epoch 83/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.2831 - accuracy: 0.8636\n",
            "Epoch 84/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.2587 - accuracy: 0.8853\n",
            "Epoch 85/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.2357 - accuracy: 0.9134\n",
            "Epoch 86/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2637 - accuracy: 0.8896\n",
            "Epoch 87/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.2464 - accuracy: 0.8810\n",
            "Epoch 88/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.2830 - accuracy: 0.8593\n",
            "Epoch 89/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.2314 - accuracy: 0.8918\n",
            "Epoch 90/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.2424 - accuracy: 0.8961\n",
            "Epoch 91/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2509 - accuracy: 0.9069\n",
            "Epoch 92/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.2479 - accuracy: 0.9156\n",
            "Epoch 93/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.2512 - accuracy: 0.8961\n",
            "Epoch 94/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2266 - accuracy: 0.9091\n",
            "Epoch 95/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2229 - accuracy: 0.9199\n",
            "Epoch 96/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.2258 - accuracy: 0.8939\n",
            "Epoch 97/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.2208 - accuracy: 0.9004\n",
            "Epoch 98/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.2440 - accuracy: 0.9026\n",
            "Epoch 99/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.2322 - accuracy: 0.9113\n",
            "Epoch 100/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.2145 - accuracy: 0.9048\n",
            "Epoch 101/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.2170 - accuracy: 0.9286\n",
            "Epoch 102/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.2240 - accuracy: 0.9199\n",
            "Epoch 103/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.2102 - accuracy: 0.9156\n",
            "Epoch 104/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.2210 - accuracy: 0.9177\n",
            "Epoch 105/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.2485 - accuracy: 0.9026\n",
            "Epoch 106/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2275 - accuracy: 0.8983\n",
            "Epoch 107/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.2551 - accuracy: 0.9221\n",
            "Epoch 108/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2362 - accuracy: 0.8983\n",
            "Epoch 109/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.2374 - accuracy: 0.8961\n",
            "Epoch 110/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2729 - accuracy: 0.9026\n",
            "Epoch 111/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2128 - accuracy: 0.9004\n",
            "Epoch 112/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2646 - accuracy: 0.9004\n",
            "Epoch 113/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.2020 - accuracy: 0.9113\n",
            "Epoch 114/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1780 - accuracy: 0.9199\n",
            "Epoch 115/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1908 - accuracy: 0.9221\n",
            "Epoch 116/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1938 - accuracy: 0.9351\n",
            "Epoch 117/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.2080 - accuracy: 0.9177\n",
            "Epoch 118/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1767 - accuracy: 0.9286\n",
            "Epoch 119/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1810 - accuracy: 0.9329\n",
            "Epoch 120/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.2061 - accuracy: 0.9242\n",
            "Epoch 121/300\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 0.1548 - accuracy: 0.9416\n",
            "Epoch 122/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1971 - accuracy: 0.9264\n",
            "Epoch 123/300\n",
            "10/10 [==============================] - 0s 26ms/step - loss: 0.1538 - accuracy: 0.9416\n",
            "Epoch 124/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1613 - accuracy: 0.9372\n",
            "Epoch 125/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1813 - accuracy: 0.9307\n",
            "Epoch 126/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1753 - accuracy: 0.9264\n",
            "Epoch 127/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2041 - accuracy: 0.9329\n",
            "Epoch 128/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1303 - accuracy: 0.9502\n",
            "Epoch 129/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1457 - accuracy: 0.9351\n",
            "Epoch 130/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1457 - accuracy: 0.9437\n",
            "Epoch 131/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1296 - accuracy: 0.9416\n",
            "Epoch 132/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1787 - accuracy: 0.9372\n",
            "Epoch 133/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1697 - accuracy: 0.9307\n",
            "Epoch 134/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1640 - accuracy: 0.9416\n",
            "Epoch 135/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1688 - accuracy: 0.9329\n",
            "Epoch 136/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1297 - accuracy: 0.9567\n",
            "Epoch 137/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1317 - accuracy: 0.9545\n",
            "Epoch 138/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1522 - accuracy: 0.9351\n",
            "Epoch 139/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1332 - accuracy: 0.9459\n",
            "Epoch 140/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1288 - accuracy: 0.9459\n",
            "Epoch 141/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1789 - accuracy: 0.9286\n",
            "Epoch 142/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1590 - accuracy: 0.9351\n",
            "Epoch 143/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1340 - accuracy: 0.9437\n",
            "Epoch 144/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1217 - accuracy: 0.9567\n",
            "Epoch 145/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1505 - accuracy: 0.9502\n",
            "Epoch 146/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1494 - accuracy: 0.9394\n",
            "Epoch 147/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1156 - accuracy: 0.9502\n",
            "Epoch 148/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1216 - accuracy: 0.9589\n",
            "Epoch 149/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1252 - accuracy: 0.9545\n",
            "Epoch 150/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1491 - accuracy: 0.9437\n",
            "Epoch 151/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.1184 - accuracy: 0.9502\n",
            "Epoch 152/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.1406 - accuracy: 0.9545\n",
            "Epoch 153/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.1403 - accuracy: 0.9567\n",
            "Epoch 154/300\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.1722 - accuracy: 0.9459\n",
            "Epoch 155/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.1372 - accuracy: 0.9459\n",
            "Epoch 156/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.1661 - accuracy: 0.9372\n",
            "Epoch 157/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.1450 - accuracy: 0.9459\n",
            "Epoch 158/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0917 - accuracy: 0.9697\n",
            "Epoch 159/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.1608 - accuracy: 0.9416\n",
            "Epoch 160/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.1304 - accuracy: 0.9329\n",
            "Epoch 161/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.1196 - accuracy: 0.9437\n",
            "Epoch 162/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.1184 - accuracy: 0.9437\n",
            "Epoch 163/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.1508 - accuracy: 0.9481\n",
            "Epoch 164/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1582 - accuracy: 0.9286\n",
            "Epoch 165/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1142 - accuracy: 0.9545\n",
            "Epoch 166/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1072 - accuracy: 0.9632\n",
            "Epoch 167/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1050 - accuracy: 0.9632\n",
            "Epoch 168/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1474 - accuracy: 0.9589\n",
            "Epoch 169/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.0948 - accuracy: 0.9675\n",
            "Epoch 170/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1081 - accuracy: 0.9567\n",
            "Epoch 171/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1003 - accuracy: 0.9567\n",
            "Epoch 172/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1208 - accuracy: 0.9675\n",
            "Epoch 173/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1376 - accuracy: 0.9632\n",
            "Epoch 174/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1345 - accuracy: 0.9610\n",
            "Epoch 175/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0956 - accuracy: 0.9567\n",
            "Epoch 176/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1050 - accuracy: 0.9567\n",
            "Epoch 177/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0924 - accuracy: 0.9675\n",
            "Epoch 178/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0911 - accuracy: 0.9740\n",
            "Epoch 179/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0625 - accuracy: 0.9740\n",
            "Epoch 180/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1086 - accuracy: 0.9524\n",
            "Epoch 181/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1247 - accuracy: 0.9567\n",
            "Epoch 182/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1102 - accuracy: 0.9481\n",
            "Epoch 183/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1399 - accuracy: 0.9524\n",
            "Epoch 184/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.0959 - accuracy: 0.9654\n",
            "Epoch 185/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1105 - accuracy: 0.9610\n",
            "Epoch 186/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1059 - accuracy: 0.9567\n",
            "Epoch 187/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.0854 - accuracy: 0.9632\n",
            "Epoch 188/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1073 - accuracy: 0.9675\n",
            "Epoch 189/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.1213 - accuracy: 0.9632\n",
            "Epoch 190/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0813 - accuracy: 0.9632\n",
            "Epoch 191/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1233 - accuracy: 0.9502\n",
            "Epoch 192/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0946 - accuracy: 0.9675\n",
            "Epoch 193/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0882 - accuracy: 0.9632\n",
            "Epoch 194/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0739 - accuracy: 0.9697\n",
            "Epoch 195/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1257 - accuracy: 0.9589\n",
            "Epoch 196/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0971 - accuracy: 0.9654\n",
            "Epoch 197/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0972 - accuracy: 0.9632\n",
            "Epoch 198/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0876 - accuracy: 0.9589\n",
            "Epoch 199/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0968 - accuracy: 0.9719\n",
            "Epoch 200/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.0818 - accuracy: 0.9654\n",
            "Epoch 201/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0857 - accuracy: 0.9805\n",
            "Epoch 202/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1011 - accuracy: 0.9654\n",
            "Epoch 203/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0894 - accuracy: 0.9632\n",
            "Epoch 204/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1024 - accuracy: 0.9610\n",
            "Epoch 205/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0703 - accuracy: 0.9632\n",
            "Epoch 206/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1261 - accuracy: 0.9589\n",
            "Epoch 207/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0796 - accuracy: 0.9697\n",
            "Epoch 208/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1069 - accuracy: 0.9762\n",
            "Epoch 209/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0690 - accuracy: 0.9762\n",
            "Epoch 210/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0696 - accuracy: 0.9719\n",
            "Epoch 211/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0955 - accuracy: 0.9697\n",
            "Epoch 212/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1070 - accuracy: 0.9762\n",
            "Epoch 213/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0844 - accuracy: 0.9719\n",
            "Epoch 214/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0758 - accuracy: 0.9675\n",
            "Epoch 215/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0622 - accuracy: 0.9805\n",
            "Epoch 216/300\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 0.1071 - accuracy: 0.9610\n",
            "Epoch 217/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1052 - accuracy: 0.9675\n",
            "Epoch 218/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0595 - accuracy: 0.9805\n",
            "Epoch 219/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0461 - accuracy: 0.9848\n",
            "Epoch 220/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0951 - accuracy: 0.9654\n",
            "Epoch 221/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.0943 - accuracy: 0.9632\n",
            "Epoch 222/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.1138 - accuracy: 0.9654\n",
            "Epoch 223/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.0765 - accuracy: 0.9589\n",
            "Epoch 224/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0798 - accuracy: 0.9610\n",
            "Epoch 225/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0514 - accuracy: 0.9784\n",
            "Epoch 226/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0477 - accuracy: 0.9762\n",
            "Epoch 227/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.0739 - accuracy: 0.9719\n",
            "Epoch 228/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.0482 - accuracy: 0.9848\n",
            "Epoch 229/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0445 - accuracy: 0.9870\n",
            "Epoch 230/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0539 - accuracy: 0.9827\n",
            "Epoch 231/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0652 - accuracy: 0.9675\n",
            "Epoch 232/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0616 - accuracy: 0.9762\n",
            "Epoch 233/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0791 - accuracy: 0.9784\n",
            "Epoch 234/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0680 - accuracy: 0.9697\n",
            "Epoch 235/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0873 - accuracy: 0.9654\n",
            "Epoch 236/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.0618 - accuracy: 0.9805\n",
            "Epoch 237/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0921 - accuracy: 0.9675\n",
            "Epoch 238/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0681 - accuracy: 0.9784\n",
            "Epoch 239/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0987 - accuracy: 0.9805\n",
            "Epoch 240/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.0762 - accuracy: 0.9784\n",
            "Epoch 241/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0636 - accuracy: 0.9784\n",
            "Epoch 242/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0844 - accuracy: 0.9740\n",
            "Epoch 243/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0780 - accuracy: 0.9697\n",
            "Epoch 244/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0675 - accuracy: 0.9740\n",
            "Epoch 245/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0788 - accuracy: 0.9719\n",
            "Epoch 246/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0404 - accuracy: 0.9784\n",
            "Epoch 247/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0488 - accuracy: 0.9827\n",
            "Epoch 248/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0969 - accuracy: 0.9697\n",
            "Epoch 249/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0809 - accuracy: 0.9654\n",
            "Epoch 250/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0771 - accuracy: 0.9827\n",
            "Epoch 251/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.0746 - accuracy: 0.9675\n",
            "Epoch 252/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0825 - accuracy: 0.9762\n",
            "Epoch 253/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1361 - accuracy: 0.9589\n",
            "Epoch 254/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.1033 - accuracy: 0.9524\n",
            "Epoch 255/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0897 - accuracy: 0.9675\n",
            "Epoch 256/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0695 - accuracy: 0.9719\n",
            "Epoch 257/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0613 - accuracy: 0.9827\n",
            "Epoch 258/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0964 - accuracy: 0.9675\n",
            "Epoch 259/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0589 - accuracy: 0.9784\n",
            "Epoch 260/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.0602 - accuracy: 0.9827\n",
            "Epoch 261/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0687 - accuracy: 0.9697\n",
            "Epoch 262/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0641 - accuracy: 0.9675\n",
            "Epoch 263/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0447 - accuracy: 0.9805\n",
            "Epoch 264/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0912 - accuracy: 0.9675\n",
            "Epoch 265/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0712 - accuracy: 0.9762\n",
            "Epoch 266/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0408 - accuracy: 0.9805\n",
            "Epoch 267/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0591 - accuracy: 0.9805\n",
            "Epoch 268/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0408 - accuracy: 0.9805\n",
            "Epoch 269/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0329 - accuracy: 0.9892\n",
            "Epoch 270/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0743 - accuracy: 0.9784\n",
            "Epoch 271/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.0472 - accuracy: 0.9784\n",
            "Epoch 272/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0627 - accuracy: 0.9805\n",
            "Epoch 273/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0645 - accuracy: 0.9805\n",
            "Epoch 274/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.0312 - accuracy: 0.9892\n",
            "Epoch 275/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.0457 - accuracy: 0.9762\n",
            "Epoch 276/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.0551 - accuracy: 0.9697\n",
            "Epoch 277/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0527 - accuracy: 0.9762\n",
            "Epoch 278/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0692 - accuracy: 0.9762\n",
            "Epoch 279/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0783 - accuracy: 0.9610\n",
            "Epoch 280/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.0852 - accuracy: 0.9610\n",
            "Epoch 281/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0723 - accuracy: 0.9697\n",
            "Epoch 282/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0805 - accuracy: 0.9784\n",
            "Epoch 283/300\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 0.0545 - accuracy: 0.9870\n",
            "Epoch 284/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.0632 - accuracy: 0.9740\n",
            "Epoch 285/300\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 0.0528 - accuracy: 0.9805\n",
            "Epoch 286/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0484 - accuracy: 0.9827\n",
            "Epoch 287/300\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 0.0947 - accuracy: 0.9654\n",
            "Epoch 288/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0471 - accuracy: 0.9827\n",
            "Epoch 289/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0524 - accuracy: 0.9848\n",
            "Epoch 290/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.0534 - accuracy: 0.9805\n",
            "Epoch 291/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0626 - accuracy: 0.9827\n",
            "Epoch 292/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0644 - accuracy: 0.9805\n",
            "Epoch 293/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.0330 - accuracy: 0.9805\n",
            "Epoch 294/300\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.1001 - accuracy: 0.9632\n",
            "Epoch 295/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0542 - accuracy: 0.9848\n",
            "Epoch 296/300\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.0678 - accuracy: 0.9740\n",
            "Epoch 297/300\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0695 - accuracy: 0.9805\n",
            "Epoch 298/300\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0528 - accuracy: 0.9848\n",
            "Epoch 299/300\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.0424 - accuracy: 0.9848\n",
            "Epoch 300/300\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.0637 - accuracy: 0.9762\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 0.0044 - accuracy: 1.0000\n",
            "\n",
            "\n",
            "Test Loss 0.004414084134623408, Test Accuracy 100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDEbE2CbVZns",
        "colab_type": "code",
        "outputId": "e1c6f073-637c-4da1-da0f-3db36a773330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# Show some results\n",
        "predictions = model.predict(test_data)\n",
        "for prediction, CHD in zip(predictions[:50], list(test_data)[0][1][:50]):\n",
        "  print(\"Prediction of CGH: {:.2%}\".format(prediction[0]) + \" Actual Outcome: \",\n",
        "        (\"Postive\" if bool(CHD) else \"Negative\"))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction of CGH: 99.96% Actual Outcome:  Postive\n",
            "Prediction of CGH: 100.00% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.00% Actual Outcome:  Postive\n",
            "Prediction of CGH: 0.90% Actual Outcome:  Negative\n",
            "Prediction of CGH: 99.99% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.01% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.11% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.19% Actual Outcome:  Negative\n",
            "Prediction of CGH: 99.96% Actual Outcome:  Postive\n",
            "Prediction of CGH: 1.04% Actual Outcome:  Negative\n",
            "Prediction of CGH: 99.96% Actual Outcome:  Negative\n",
            "Prediction of CGH: 98.96% Actual Outcome:  Postive\n",
            "Prediction of CGH: 0.00% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.00% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.00% Actual Outcome:  Negative\n",
            "Prediction of CGH: 1.34% Actual Outcome:  Postive\n",
            "Prediction of CGH: 0.26% Actual Outcome:  Negative\n",
            "Prediction of CGH: 99.24% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.00% Actual Outcome:  Postive\n",
            "Prediction of CGH: 99.99% Actual Outcome:  Negative\n",
            "Prediction of CGH: 95.41% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.00% Actual Outcome:  Negative\n",
            "Prediction of CGH: 100.00% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.00% Actual Outcome:  Negative\n",
            "Prediction of CGH: 99.90% Actual Outcome:  Postive\n",
            "Prediction of CGH: 0.02% Actual Outcome:  Negative\n",
            "Prediction of CGH: 100.00% Actual Outcome:  Negative\n",
            "Prediction of CGH: 1.14% Actual Outcome:  Postive\n",
            "Prediction of CGH: 3.32% Actual Outcome:  Postive\n",
            "Prediction of CGH: 0.00% Actual Outcome:  Postive\n",
            "Prediction of CGH: 2.21% Actual Outcome:  Negative\n",
            "Prediction of CGH: 100.00% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.00% Actual Outcome:  Negative\n",
            "Prediction of CGH: 100.00% Actual Outcome:  Postive\n",
            "Prediction of CGH: 2.00% Actual Outcome:  Postive\n",
            "Prediction of CGH: 0.00% Actual Outcome:  Postive\n",
            "Prediction of CGH: 100.00% Actual Outcome:  Postive\n",
            "Prediction of CGH: 0.62% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.00% Actual Outcome:  Postive\n",
            "Prediction of CGH: 99.99% Actual Outcome:  Postive\n",
            "Prediction of CGH: 99.96% Actual Outcome:  Postive\n",
            "Prediction of CGH: 0.10% Actual Outcome:  Postive\n",
            "Prediction of CGH: 99.91% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.02% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.01% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.01% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.09% Actual Outcome:  Postive\n",
            "Prediction of CGH: 0.00% Actual Outcome:  Negative\n",
            "Prediction of CGH: 97.55% Actual Outcome:  Negative\n",
            "Prediction of CGH: 0.80% Actual Outcome:  Negative\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}